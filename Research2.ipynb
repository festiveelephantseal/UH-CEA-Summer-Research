{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60b728a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94b49250",
   "metadata": {},
   "outputs": [],
   "source": [
    "census = pd.read_csv(\"cc-est2024-alldata.csv\", encoding=\"latin-1\")\n",
    "\n",
    "census = census[(census[\"YEAR\"] == 3) & census[\"AGEGRP\"] == 0]\n",
    "\n",
    "census[\"LocationID\"] = census[\"STATE\"].astype(str).str.zfill(2) + census[\"COUNTY\"].astype(str).str.zfill(3)\n",
    "\n",
    "census[\"Pct_Black\"] = census[\"BA_MALE\"] + census[\"BA_FEMALE\"]\n",
    "census[\"Pct_Hispanic\"] = census[\"H_MALE\"] + census[\"H_FEMALE\"]\n",
    "census[\"Pct_White\"] = census[\"WA_MALE\"] + census[\"WA_FEMALE\"]\n",
    "census[\"Pct_Asian\"] = census[\"AA_MALE\"] + census[\"AA_FEMALE\"]\n",
    "\n",
    "census[\"Pct_Black\"] = census[\"Pct_Black\"] / census[\"TOT_POP\"] \n",
    "census[\"Pct_Hispanic\"] = census[\"Pct_Hispanic\"] / census[\"TOT_POP\"]\n",
    "census[\"Pct_White\"] = census[\"Pct_White\"] / census[\"TOT_POP\"]\n",
    "census[\"Pct_Asian\"] = census[\"Pct_Asian\"] / census[\"TOT_POP\"]\n",
    "\n",
    "census[\"LocationID\"] = census[\"LocationID\"].astype(str)\n",
    "\n",
    "census = census[[\"LocationID\", \"Pct_Black\", \"Pct_Hispanic\", \"Pct_Asian\", \"Pct_White\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7688580d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Data_Value</th>\n",
       "      <th>Data_Value_Footnote_Symbol</th>\n",
       "      <th>Data_Value_Footnote</th>\n",
       "      <th>Low_Confidence_Limit</th>\n",
       "      <th>High_Confidence_Limit</th>\n",
       "      <th>TotalPopulation</th>\n",
       "      <th>LocationID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>187656.000000</td>\n",
       "      <td>187656.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>187656.000000</td>\n",
       "      <td>187656.000000</td>\n",
       "      <td>1.876560e+05</td>\n",
       "      <td>187656.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2018.732020</td>\n",
       "      <td>31.724481</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.748927</td>\n",
       "      <td>33.756836</td>\n",
       "      <td>2.078160e+05</td>\n",
       "      <td>30356.043484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.442908</td>\n",
       "      <td>24.828821</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.479171</td>\n",
       "      <td>25.216044</td>\n",
       "      <td>5.875963e+06</td>\n",
       "      <td>15202.499969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2018.000000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>8.600000e+01</td>\n",
       "      <td>59.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2018.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>14.100000</td>\n",
       "      <td>1.080300e+04</td>\n",
       "      <td>18167.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2019.000000</td>\n",
       "      <td>23.800000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.700000</td>\n",
       "      <td>26.800000</td>\n",
       "      <td>2.561600e+04</td>\n",
       "      <td>29159.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2019.000000</td>\n",
       "      <td>40.800000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.100000</td>\n",
       "      <td>43.600000</td>\n",
       "      <td>6.705500e+04</td>\n",
       "      <td>45089.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2019.000000</td>\n",
       "      <td>93.200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92.600000</td>\n",
       "      <td>93.700000</td>\n",
       "      <td>3.282395e+08</td>\n",
       "      <td>56045.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Year     Data_Value  Data_Value_Footnote_Symbol  \\\n",
       "count  187656.000000  187656.000000                         0.0   \n",
       "mean     2018.732020      31.724481                         NaN   \n",
       "std         0.442908      24.828821                         NaN   \n",
       "min      2018.000000       1.800000                         NaN   \n",
       "25%      2018.000000      12.600000                         NaN   \n",
       "50%      2019.000000      23.800000                         NaN   \n",
       "75%      2019.000000      40.800000                         NaN   \n",
       "max      2019.000000      93.200000                         NaN   \n",
       "\n",
       "       Data_Value_Footnote  Low_Confidence_Limit  High_Confidence_Limit  \\\n",
       "count                  0.0         187656.000000          187656.000000   \n",
       "mean                   NaN             29.748927              33.756836   \n",
       "std                    NaN             24.479171              25.216044   \n",
       "min                    NaN              1.600000               2.000000   \n",
       "25%                    NaN             10.800000              14.100000   \n",
       "50%                    NaN             20.700000              26.800000   \n",
       "75%                    NaN             39.100000              43.600000   \n",
       "max                    NaN             92.600000              93.700000   \n",
       "\n",
       "       TotalPopulation     LocationID  \n",
       "count     1.876560e+05  187656.000000  \n",
       "mean      2.078160e+05   30356.043484  \n",
       "std       5.875963e+06   15202.499969  \n",
       "min       8.600000e+01      59.000000  \n",
       "25%       1.080300e+04   18167.000000  \n",
       "50%       2.561600e+04   29159.000000  \n",
       "75%       6.705500e+04   45089.000000  \n",
       "max       3.282395e+08   56045.000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"PLACES__Local_Data_for_Better_Health__County_Data_2021_release_20250801.csv\", engine=\"python\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "733c53cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"Data_Value_Type\"] == \"Crude prevalence\"].pivot(index=[\"LocationID\", \"LocationName\"], columns=\"Short_Question_Text\", values=\"Data_Value\").reset_index().dropna()\n",
    "df[\"LocationID\"] = df[\"LocationID\"].astype(str)\n",
    "df = pd.merge(df, census, on=\"LocationID\", how=\"inner\").reset_index().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb3023b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the 2020 and 2021 median income CSVs\n",
    "income_2020 = pd.read_csv(\"county_median_income_2020.csv\", header=None, \n",
    "                         names=[\"median_income_2020\", \"name\", \"state\", \"county\"])\n",
    "income_2021 = pd.read_csv(\"county_median_income_2021.csv\", header=None, \n",
    "                         names=[\"median_income_2021\", \"name\", \"state\", \"county\"])\n",
    "\n",
    "# Create LocationID (5-digit FIPS) for both datasets\n",
    "income_2020[\"LocationID\"] = income_2020[\"state\"].astype(str).str.zfill(2) + income_2020[\"county\"].astype(str).str.zfill(3)\n",
    "income_2021[\"LocationID\"] = income_2021[\"state\"].astype(str).str.zfill(2) + income_2021[\"county\"].astype(str).str.zfill(3)\n",
    "\n",
    "# Clean up - keep only what we need\n",
    "income_2020 = income_2020[[\"LocationID\", \"median_income_2020\"]]\n",
    "income_2021 = income_2021[[\"LocationID\", \"median_income_2021\"]]\n",
    "\n",
    "# Convert income columns to numeric (in case they're strings)\n",
    "income_2020[\"median_income_2020\"] = pd.to_numeric(income_2020[\"median_income_2020\"], errors='coerce')\n",
    "income_2021[\"median_income_2021\"] = pd.to_numeric(income_2021[\"median_income_2021\"], errors='coerce')\n",
    "\n",
    "# Merge both income datasets\n",
    "income_combined = pd.merge(income_2020, income_2021, on=\"LocationID\", how=\"outer\")\n",
    "\n",
    "# Now merge with your existing dataframe to get both columns\n",
    "df = pd.merge(df, income_combined, on=\"LocationID\", how=\"inner\").reset_index().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86bbf1c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>LocationID</th>\n",
       "      <th>LocationName</th>\n",
       "      <th>All Teeth Lost</th>\n",
       "      <th>Annual Checkup</th>\n",
       "      <th>Arthritis</th>\n",
       "      <th>Binge Drinking</th>\n",
       "      <th>COPD</th>\n",
       "      <th>Cancer (except skin)</th>\n",
       "      <th>...</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>Taking BP Medication</th>\n",
       "      <th>Pct_Black</th>\n",
       "      <th>Pct_Hispanic</th>\n",
       "      <th>Pct_Asian</th>\n",
       "      <th>Pct_White</th>\n",
       "      <th>median_income_2020_x</th>\n",
       "      <th>median_income_2021_x</th>\n",
       "      <th>median_income_2020_y</th>\n",
       "      <th>median_income_2021_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [level_0, index, LocationID, LocationName, All Teeth Lost, Annual Checkup, Arthritis, Binge Drinking, COPD, Cancer (except skin), Cervical Cancer Screening, Cholesterol Screening, Chronic Kidney Disease, Colorectal Cancer Screening, Core preventive services for older men, Core preventive services for older women, Coronary Heart Disease, Current Asthma, Current Smoking, Dental Visit, Depression, Diabetes, General Health, Health Insurance, High Blood Pressure, High Cholesterol, Mammography, Mental Health, Obesity, Physical Health, Physical Inactivity, Sleep <7 hours, Stroke, Taking BP Medication, Pct_Black, Pct_Hispanic, Pct_Asian, Pct_White, median_income_2020_x, median_income_2021_x, median_income_2020_y, median_income_2021_y]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 42 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becdd2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample LocationID values in df:\n",
      "Series([], Name: LocationID, dtype: object)\n",
      "Total rows in df before merge: 0\n",
      "\n",
      "Sample LocationID values in income data:\n",
      "57982  Autauga County     01nan\n",
      "61756  Baldwin County     03nan\n",
      "34990  Barbour County     05nan\n",
      "51721  Bibb County        07nan\n",
      "48922  Blount County      09nan\n",
      "33866  Bullock County     11nan\n",
      "44850  Butler County      13nan\n",
      "50128  Calhoun County     15nan\n",
      "29844  Lewis County      135nan\n",
      "42231  Lincoln County    137nan\n",
      "Name: LocationID, dtype: object\n",
      "Total rows in income_2020: 3221\n",
      "\n",
      "Number of matching LocationIDs: 0\n",
      "First few matching LocationIDs:\n",
      "[]\n",
      "\n",
      "Checking data types and formats:\n",
      "df LocationID type: object\n",
      "income LocationID type: object\n"
     ]
    }
   ],
   "source": [
    "# Let's see what happened to your main dataframe\n",
    "print(\"Checking each step of your pipeline:\")\n",
    "print(f\"census rows: {len(census) if 'census' in globals() else 'census not defined'}\")\n",
    "print(f\"df after PLACES merge: {len(df)}\")\n",
    "\n",
    "# Check if the PLACES data loaded correctly\n",
    "places_df = pd.read_csv(\"PLACES__Local_Data_for_Better_Health__County_Data_2022_release.csv\", engine=\"python\")\n",
    "print(f\"Original PLACES rows: {len(places_df)}\")\n",
    "places_filtered = places_df[places_df[\"Data_Value_Type\"] == \"Crude prevalence\"]\n",
    "print(f\"After filtering: {len(places_filtered)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca80c488",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 6)) while a minimum of 1 is required by StandardScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[32m      8\u001b[39m scaler = StandardScaler()\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m X_scaled_array = \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m X_scaled = pd.DataFrame(X_scaled_array, columns=X.columns, index=X.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/UH-CEA-Summer-Research/.venv/lib/python3.12/site-packages/sklearn/utils/_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/UH-CEA-Summer-Research/.venv/lib/python3.12/site-packages/sklearn/base.py:892\u001b[39m, in \u001b[36mTransformerMixin.fit_transform\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    877\u001b[39m         warnings.warn(\n\u001b[32m    878\u001b[39m             (\n\u001b[32m    879\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) has a `transform`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    887\u001b[39m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m    888\u001b[39m         )\n\u001b[32m    890\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    891\u001b[39m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m892\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m.transform(X)\n\u001b[32m    893\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    894\u001b[39m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[32m    895\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fit(X, y, **fit_params).transform(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/UH-CEA-Summer-Research/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_data.py:907\u001b[39m, in \u001b[36mStandardScaler.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    905\u001b[39m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[32m    906\u001b[39m \u001b[38;5;28mself\u001b[39m._reset()\n\u001b[32m--> \u001b[39m\u001b[32m907\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/UH-CEA-Summer-Research/.venv/lib/python3.12/site-packages/sklearn/base.py:1363\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1356\u001b[39m     estimator._validate_params()\n\u001b[32m   1358\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1359\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1360\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1361\u001b[39m     )\n\u001b[32m   1362\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1363\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/UH-CEA-Summer-Research/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_data.py:943\u001b[39m, in \u001b[36mStandardScaler.partial_fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    911\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[32m    912\u001b[39m \n\u001b[32m    913\u001b[39m \u001b[33;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m \u001b[33;03m    Fitted scaler.\u001b[39;00m\n\u001b[32m    941\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    942\u001b[39m first_call = \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mn_samples_seen_\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    951\u001b[39m n_features = X.shape[\u001b[32m1\u001b[39m]\n\u001b[32m    953\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/UH-CEA-Summer-Research/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2954\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2952\u001b[39m         out = X, y\n\u001b[32m   2953\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2954\u001b[39m     out = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2955\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2956\u001b[39m     out = _check_y(y, **check_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/UH-CEA-Summer-Research/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:1128\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1126\u001b[39m     n_samples = _num_samples(array)\n\u001b[32m   1127\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_samples < ensure_min_samples:\n\u001b[32m-> \u001b[39m\u001b[32m1128\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1129\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m) while a\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1130\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1131\u001b[39m             % (n_samples, array.shape, ensure_min_samples, context)\n\u001b[32m   1132\u001b[39m         )\n\u001b[32m   1134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array.ndim == \u001b[32m2\u001b[39m:\n\u001b[32m   1135\u001b[39m     n_features = array.shape[\u001b[32m1\u001b[39m]\n",
      "\u001b[31mValueError\u001b[39m: Found array with 0 sample(s) (shape=(0, 6)) while a minimum of 1 is required by StandardScaler."
     ]
    }
   ],
   "source": [
    "X = df[[\"Obesity\", \"median_income_2021\", \"Pct_Black\", \"Pct_Hispanic\", \"Pct_White\", \"Pct_Asian\"]] \n",
    "y = df[\"Diabetes\"] / 100\n",
    "\n",
    "X[\"median_income_2021\"] = X[\"median_income_2021\"] / 1000\n",
    "X[\"Obesity\"] = X[\"Obesity\"] / 100\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled_array = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled_array, columns=X.columns, index=X.index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
